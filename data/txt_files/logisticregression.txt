When you run an ESM on the training data and look at the R^2 value, what does the value represent? Since the validation data is contained in a separate data set, I'm not sure how it's being evaluated.
Isn't it just the amount of variation in the training data that is explained by the model? 
So, it just looks at the training data and is not related to the predictions?
Right. Just as there is a mape for both the training and validation data set. There would be for R^2 as well. But the validation one is what we would care about when predicting. This goes back to what labarr was talking about between the difference in goodness of fit and accuracy. 
Mape is obviously talking about time series, but it's the same kind of logic I'm fairly certain. 
Ok, I think that makes sense. Thanks!
Hey everyone! Above is a useful diagram to help you understand the model implications of the CMH and BDT tests from logistic regression. The diagram assumes you are trying to model 'y' based on 'x' and you think 'z' might be a confounding variable. Hope this helps connect some dots! Let me know if you have any questions.
Confusion at end of class Monday: new model (serious only) was being taught on the validation data. To find out how to teach it on training data then output the predictions so you can compare these predictions, check out the SAS e learning: Predictive Modeling Using Logistic Regression&gt;Lesson 4&gt; comparing RoC curves to measure model performance....5 min video makes it super easy
Has anyone received feedback for the logistic homework ?
<@U1KQ9523D> not yet
No.
<@U1KQ9523D>: dr Simmons mentioned she would be spending today grading 
anyone else having trouble running best subset models? I keep getting an error in my log that says "Blank or duplicate name or invalid subscript"
not sure if this helps, but when we removed a certain variable we did not get that error anymore
<@U1M1L5G6L> yeah, that's what I'm trying now. weird that I couldn't find any explanation of that error message online
Does it have to do with categorical vs continuous vars? Since you can't have a class statement in best subset, can you still include categorical vars? If so, does it default to effects coding?
<!channel>: logistic final project due Sunday now like time series 
<@U1KSBCBFG> good call, just fixed it on the calendar
<@U1KT9ATA5>: since she just sent out the email I think ur pretty on top it :) 
Top of it*
Has anyone done the last Questions for Thought? For #7, I am confused when she says 'Above, I went with the simpler model.' Is she referring to #3, where she asked us to leave out serious2 and slogs, as the simpler model?
Which elearning modules are useful for our Logistic class? Is it the "predictive modeling using logistic regression" or is it the end of the "intro to ANOVA, regression, and logistic regression portion". Or is it both.
<@U1L1D4WG7>: both
The intro one covers mostly what we went over in boot camp, but also some of what we went over in lr class. In Predictive Modeling, some sections may be more relevant to class topics than others, but all could be on the Business Analyst exam
Both
Thanks <@U1KSL7C10> and <@U1KRNH021> :slightly_smiling_face:
Did Dr Simmons give us an answer key for the previous assessment?
<@U1KT2JJE8>: no but it was labarrs test. Has anyone tried asking him?
I mailed her couple of hours ago.
has she responded?
No she didn't. 
*Simmon's my B!
Thanks Caroline!
<@U1L41DF9D>  is the best
No ordinal or nominal or confounding on the test  :slightly_smiling_face:
Noooo I already studied those topics.
Has anyone developed their own key to the logistic regression practice exam?
<@U1LBRCSEA> I asked Dr. Labarr for it and he told me he needed to ask Dr. Spencer for it before he sent it to me!
<@U1L41DF9D>   Why do you keep saying Spencer?
Simmons - my statistics professor  at UNCW was spencer so I've just been confusing them :see_no_evil:
<@U1L1D4WG7> -- when you say no ordinal or nominal or confounding, are yo utalking about logistic regression exam on friday, or business analyst cert?
<@U1KR9BP0E> just for the exam Friday. 
did she mention this in your class?
This is so weird.  My rap name is G-Logit.
chakin' my head (CMH)
<@U1L41DF9D>   Ask her about the key.
<@U1L6YQUSU> she's not giving it out
That probably means she'll use some questions from it for the test 
We can all build a key to it at least!
<@U1KR9BP0E> Yep she said that towards the end of our class
We really need Julia's Woo emoji right now.
<@U1L41DF9D>: ur killing it today! Thanks again!
Wait, why is 3 false?  I thought that was the whole point of using Fischer's Exact test
It's not the ONLY way (trick question)
There are other exact tests....
Thanks, just found the slide that shows some of the other exact tests.  I guess we're just expected to know that they exist :slightly_smiling_face:
The logistic regression study guide is up on the website now
So ordinal and nominal LR and confounding will not be in the exam, right? Just want to double make sure
Does anyone know what Dr. Simmons is referring to by LRT? It's number 11 in the review topics.
Likelihood ratio test possibly? 
Just a guess haha <@U1KRSV7AN> 
It's always worth a search. Thanks :slightly_smiling_face:
Yup, it's likelihood ratio test! <@U1KRSV7AN> 
thank you <@U1KRWUXFU> and <@U1L9DD1NY>!
<!channel>: anyone group studying in the library 
We're group studying in the Nash conference room, you in the library?
James and I are working in a room at the library
How long you going to be there, <@U1KT5F4SZ> ?
<@U1L7E8GSU> minimum another hour
I'm at the library
<https://quizlet.com/155050858/iaa-logistic-regression-flash-cards/> - here is the flashcards for logistic regression
You rock, <@U1L41DF9D> !! The flash cards for linear algebra were so helpful too! 
you're the best!
so does logistic regression not care for model hierarchy?
thanks!
Long shot: did anyone happen to record the logistic review sesh today? I know dr. Simmons said she'd record but it's not uploaded yet -- I've emailed her about it
It is uploaded
its up there.  9/29 review session
I'm an idiot
Thanks <@U1L7E8GSU> <@U1KT5F4SZ> 
When would we use the exact tests? Is it just when we have a small sample size or sparse data set?
<@U1KSL7L7R> For small sample sizes (&gt;20% of expected counts are less than 5)
Thank you <@U1KRE1L9M>
I'm confused about CMH vs BDT. What's the difference?
<@U1KS4UE2F> - if you're still on campus, John Betz is in nash and is pretty good at explaining it
<@U1L6YQUSU> share with the class, please!
yeah actually i'm having trouble with this one too <@U1KS4UE2F> <@U1L6YQUSU>  any help is appreciated
Let me type something up.  One moment please.
So, let's say you have 2 variables, A &amp; B and they have some sort of relationship.  We want to know if there's an association between A and B even if we break them down further into categories of a 3rd variable, C.
So we're stratifying by C.  It kind of looks like this.
Do we still have an association between A and B, even though we've split them up by C?  That's what the CMH tells us.  The Null Hyp is that there is no association between A and B.  The alternative is that there is an association between A and B.
Although you don't need to know this aspect, you're actually checking to see if there's confounding.
The BMT is a test to see if there's an interaction between A and B, which is a different thing.
The BMT tells us whether or not there's an interaction by seeing if we have common (adjusted) odds ratios in each of these little subsets
The null for BMT is that there's no interaction (i.e. you have common odds ratios)
The alternative is that there IS an interaction and you DON'T have common odds ratios.
My follow-up question is -- if there's no association in the CMH, is there still a possibility that you would use an interaction/there is a common odds ratio?
so is the logical order of testing: reject CMH -&gt; BDT?
That sounds right, <@U1KS4UE2F> .   If you reject CMH, that means there is an association, which to me means that you could go further to see if you would consider them to have an interaction.
If there's no relationship, <@U1KR9BP0E>, then I'd find it hard to believe there's an interaction
ok, got it. thanks so much <@U1L6YQUSU>!
ya thanks jbj! u keep saying bmt but i think thats a sandwich :stuck_out_tongue:
thanks John! different question for anyone: After you see the Likelihood Ratio test in the Testing Global Hypothesis table is significant, then you can check your Likelihood Estimates table for the p-values of individual predictor variables. Can someone explain when the Type 3 table comes in to play? Is it when you are dealing only with interactions?
MLT is a delicious sandwich.
<@U1L6YQUSU>: "Mutton Lettuce and Tomato?" :joy:
<@U1L3PU7BJ> if i recall, type 3 is to see if the variable is significant to the model or not. if it is, you include all levels of it
Yes <@U1L2P17DZ>.  Thanks for getting the reference. :sunglasses:
Do you think we will be alive tomorrow... It would take a miracle 
<@U1KT9ATA5> gotcha thanks
<@U1KRZT11T> Alive/Dead is binary, so you need to do an odds ratio test.
oh Lord
stahp
 <@U1L1D4WG7>  It's not binary because there is also the possibility of mostly dead
Point: <@U1KRZT11T> 
Miracle/No Miracle is also binary. Alive/Mostly Dead/Dead is ordinal then, so I'm going with Pearson. Mostly because I can't spell Hansel Gretyl.
<@U1L6YQUSU> What's BMT? Breslow day tarone?
<@U1M37FW5U>: BMT is a sandwich at Subway 
BMT = sandwich at subway
BDT = Breslow Day Tarone
I think this mutton conversation may be the best set of references and stats jokes intertwined that we've ever accomplished. Well done everyone 
<@U1L1D4WG7> <https://media.giphy.com/media/GfIcmKflAHAmk/giphy.gif>
Sooooo what's the difference between quasi complete separation and complete ?
Like how come that is quasi and not complete 
Also why is quasi complete separation bad? I know that it's because one variable perfectly predicts something but why is that necessarily bad? Sorry rambling here
quasi complete only separates a zero from the rest of the variables. It's "partly" separated. Complete separation I think happens in binary when all of the 1's are in one category, and all of the 0's are in another category.
I'm  not sure that is actually a good explanation
makes sense to me
If you have complete separation, you don't need to model it. it is already perfectly modeled.
Gotcha okay thank you!
Anyone in there library now?
<@U1L3G0YRJ>: I'm heading there in 10 minutes! 
I'm setting up camp on the fourth floor overlooking the lake if anyone wants to join!
Bunch of us in the fishbowl
Did Dr.Simmons say if the test was going to be a mix of MC, T/F and short answer or is it all going to be short answer like the QFTs?
I think she said it's going to be all the above.  MC, T/F, and short answer
Ok perfect, thank you! <@U1L7E8GSU>
I believe you look at llogl! <@U1L9DD1NY>
look at the variable that is x * log(x).  If it's p value is significant than the logit calculation is not appropriate and you need to calculate a different logit.
Today Dr Simmons talked about some kind of test to tell whether you should use an ordinal categorical variable or quantitative variable.  Can someone point to me where in the slides and/or videos this is discussed?  I can't seem to find that in my notes.
<@U1N43E3E0>  I think what you're looking for is the LRT (likelihood ratio test).
Oh I see <@U1MBURU4D> .  Thanks.  Somehow I had that written down as a way to test whether a parsimonious model is more appropriate.  Looks like it can be used for both.
<@U1N43E3E0> Yep, the LRT can be used for both purposes. Think of it like this: When you're testing whether a variable should be continuous or categorical, you're testing a simpler model (variable is continuous) against a more complex one (variable is categorical)
In the section on "how to test nested models", she gives us that 5 step formula and in the following slide she uses gives an example. Except, in the example it looks like she uses the AIC instead of the -2LogL - that was a mistake right? I though she mentioned it but wasn't sure.
Never mind. It was definitely a mistake. She's got another example where she uses only the -2LogL in the formula.
<@U1KRHCR7T>, are you sure the continuous model is the simpler one and not the more complex one? To make them into nested models wouldn't you have to bin the continuous variable into a larger number of bins?
<@U1KT2JJE8> In my notes from today's review session, I wrote "When a variable is treated as continuous, this is a subset of the model where it is categorical (continuous assumes constant odds ratio, whereas categorical allows the odds ratio to change)"
but perhaps I misheard something...
I agree with <@U1KRHCR7T>.  The reduced model is the one where the variable is continuous.
so is the null that simpler model is best, which is also saying the variable should be treated as continuous?
Yes, <@U1LBRCSEA>
but if it's a numerical continuous variable vs. a bunch of ordinal variables, how would the computer know that it was a nested model? Couldn't you then extend the test to comparing non-nested models where there one had a continuous variable and the other had a (completely unrelated) ordinal variable?
Or are we just taking the same old binned categorical variable and not putting it in the class statement and that's what makes it seem like a nested model, computationally speaking?
The computer doesn't know they're nested models because you're running both separately.  It's up to you to know that.
I think the point of this test is just to give you a way to decide how you should treat a specific variable.
so the requirement to compare models with LRT only if one is nested is really a way of trying to ensure the ROC curves don't cross?
<@U1KT2JJE8> as far as i understood, continuous variable takes only 1 degree of freedom from the model, where as categorical would take away "the number of levels minus one" degrees of freedom. Hence, the continuous model is "simpler". And LRT can tell you if the additional variables are significant or not.
I'm with her :arrow_up:
Agreed. And if you go down a slide or two, she's got another example where it's shown correctly. I think the initial slides she posted had a mistake in the first example.
I remember someone was talking about confounding....  just stumbled upon the fact that confounding and interaction are mutually exclusive according to Dr. LaBarr's videos :slightly_smiling_face: Just FYI
lol... I just hit that part of Dr. LaBarr's videos
So if the confounding variables can't have interactions, then those are the ones where we reject H0 for the CMH, but then don't reject it for the BDT, and end up using a common odds ratio?
<@U1KT2JJE8> i think that's when this chart helps
so in your case, you'd go left then right on the tree
:bulb: thanks, <@U1KS4UE2F>!
Didn't she say "The word confounding will not show up on the test"? Right?
yes
no confounding, no youden, and binary logistic regression only.
Thanks!
did dr simmons mention today if we need to know the formulas for calculating any of the exact tests or do we just need to know when to use them?
I don't think you need to know how to calculate them
I think the only thing she expects us to be able to calculate are: odds ratios (given a two by two table), test stat with df for LRT, maybe chi-sq stats (ie. obs-exp^2/exp)?
thanks!
<@U1KQYG6SJ> <@U1KT5F4SZ>: I highly doubt we'll have to calculate chi square stats
Can you smell what the ROC is cooking?
How do we calculate the LRT for 2 models we compare? Simmons said we will have to do it by hand, right?
<@U1NUU2U9E> look at the bottom of Logan's cheat sheet
<@U1NUU2U9E> Run both models.  Subtract the -2logL terms.  Subtract the 2 Degree of Freedom terms.  Use a calculator to find the area under the chi-squared curve.
corresponding to that Chi-squared value and those degrees of freedom you just calculated
<@U1L7G5LR4> it is my clue to go to sleep...
<@U1KQGK2CB> looks you might have a typo.  Cramer's V is bounded from 0 to 1, not -1 to 1.
It's bounded from -1 to 1 when it's a 2x2 table but when you have something other than that it's from 0-1 
It was in one of LaBarr's lectures. 
Yeah I used that <@U1KSR4JP3>. But I guess we wouldn't have a 2x2 table in this case (as that would be odds ratio).  Your right <@U1N43E3E0>
I actually wasn't aware of that little nuance, so good catch <@U1KSR4JP3>.

Interestingly if you google Cramer's V, everything I can find just says it's from 0 to 1.  It seems to be a lesser known property of the statistic since I guess in practice, like <@U1KQGK2CB> said, the odds ratio is the more useful statistic for a 2x2 matrix.
Some other resources
Good evening everyone, do I need to know how to get a p-value out of my TI-89??
or can we pull our laptops out and use R?
She said we wouldnâ€™t have to calculate the p-value
sweet.
Can someone explain how we use the LRT to see if ordinal variables can be treated as categorical or continuous?
Anybody has an extra calculator? I just find that my calculator disappeared magically. Cry!
<@U1KQGK2CB> In your full model you would put the variable in the class model and in your reduced model you would not put it in the class variable. Then you would go through the LRT steps to see if there is a difference. If the difference is significant, you would treat it as continuous over categorical
so, looking at the review sheet, "2.	Understand dummy variable coding (and calculating odds ratios when using reference coding)"
re: calculating odds ratios when using reference coding, is this referring to when we have the estimates per level?
I think that's referring to calculating the odds ratio by taking e^parameter estimate
When you do reference coding the parameter estimates are the difference between the logit of the reference level and the logit of that level
cool, thanks!
<@U1M4QJD4P> no problem! I'm glad it was helpful!
