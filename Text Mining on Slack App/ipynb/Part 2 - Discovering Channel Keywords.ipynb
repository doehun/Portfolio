{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Channel Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of content in Slack. As mentioned in Part 1, there are more than 10,000 messages across 62 channels. People talk about a myriad of topics using different forms of expressions. \n",
    "\n",
    "We need an effective way to summarize the content in each channel. One way to do this is by extracting keywords that are pertinent to each channel. This will clear up a lot of the clutter - imagine getting about 10 phrases that summarize the content of each channel. One way to do this is by considering each channel as a 'document', and comparing naive counts of words in each of these documents. This is problematic however, for a number of different reasons: \n",
    "\n",
    "* There is no context in naive counts of words\n",
    "* Each word is considered equally as important to a document, even though some words may be more pertinent\n",
    "* Longer documents will contain more words and appear more pertinent\n",
    "* It captures no interdependence between words\n",
    "* There are many words that don't provide little meaning, but add to the feature space, creating the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "\n",
    "We will address some of these issues using preprocessing and Term Frequency Inverse Document Frequency ([Tf-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)). Tf-Idf gives weight to words in a document if they are frequently occuring in a document, and lessens weight if they are common across other documents. It thus outputs words that are usually relevant to a specific document that is not found much elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents ###\n",
    "> Part 1: [Analyzing Trends by Time](Part 1 - Analyzing Trends by Time.html)\n",
    "\n",
    "> **Part 2: [Discovering Channel Keywords]**\n",
    "\n",
    "> Part 3: [Channel Clustering](Part 3 - Channel Clustering.html)\n",
    "\n",
    "> Part 4: [Sentiment Analysis](Part 4 - Sentiment Analysis.html)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/concatenated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>reactions</th>\n",
       "      <th>reaction_counts</th>\n",
       "      <th>reaction_total_count</th>\n",
       "      <th>reaction_type_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>sascoding</td>\n",
       "      <td>2016-08-11 19:17:40.000268</td>\n",
       "      <td>U1KR9BP0E</td>\n",
       "      <td>&lt;@U1KQYG6SJ&gt;: I will support your continued le...</td>\n",
       "      <td>[u'joy']</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8905</th>\n",
       "      <td>social</td>\n",
       "      <td>2016-07-06 18:47:34.000013</td>\n",
       "      <td>U1L6WB44A</td>\n",
       "      <td>Dying to see The Secret Life of Pets this Frid...</td>\n",
       "      <td>[u'+1']</td>\n",
       "      <td>[5]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>general</td>\n",
       "      <td>2016-08-28 18:34:53.000034</td>\n",
       "      <td>U1KQYG6SJ</td>\n",
       "      <td>interesting that just last year, they had addr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        channel                    datetime         id  \\\n",
       "8258  sascoding  2016-08-11 19:17:40.000268  U1KR9BP0E   \n",
       "8905     social  2016-07-06 18:47:34.000013  U1L6WB44A   \n",
       "2432    general  2016-08-28 18:34:53.000034  U1KQYG6SJ   \n",
       "\n",
       "                                                   text reactions  \\\n",
       "8258  <@U1KQYG6SJ>: I will support your continued le...  [u'joy']   \n",
       "8905  Dying to see The Secret Life of Pets this Frid...   [u'+1']   \n",
       "2432  interesting that just last year, they had addr...       NaN   \n",
       "\n",
       "     reaction_counts  reaction_total_count  reaction_type_count  \n",
       "8258             [2]                   2.0                  1.0  \n",
       "8905             [5]                   5.0                  1.0  \n",
       "2432             NaN                   NaN                  NaN  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass through the *text* portion of the data, and compute Tf-Idf scores of two-word phrases in each channel. We will do some preprocessing to first remove *filler words* called stop words, such as 'and', 'to', etc. Then, we will find the 200 most important phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_vectorizer():\n",
    "    return TfidfVectorizer(input = 'content', max_features = 200, stop_words = 'english',\n",
    "                         encoding = 'utf-8', ngram_range = (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for displaying the top 10 key phrases output by tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of top phrases to display\n",
    "top_n = 10\n",
    "\n",
    "def display_scores(vectorizer, tfidf_result, top_n):\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "\n",
    "    sorted_scores = sorted(scores, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    for item in sorted_scores[:top_n]:\n",
    "        print \"{0:20} Score: {1}\".format(item[0], item[1])\n",
    "\n",
    "vectorizer = create_vectorizer()\n",
    "\n",
    "def display_channel_top(channel, top_n = top_n):\n",
    "    text = data[data['channel'] == channel]['text']\n",
    "    tfidf_result = vectorizer.fit_transform(text.values.astype('U'))\n",
    "    print \"Channel: \", channel + '\\n'\n",
    "    \n",
    "    display_scores(vectorizer, tfidf_result, top_n)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, 62 outputs would be too gaudy. In an era of information overload, you don't need to be thrown more information. I will only display results for a select channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel:  social\n",
      "\n",
      "beer garden          Score: 22.7900132132\n",
      "let know             Score: 19.5876367634\n",
      "http www             Score: 12.7183706158\n",
      "going tonight        Score: 11.635329113\n",
      "wants join           Score: 11.5523027368\n",
      "https www            Score: 8.14295801667\n",
      "just got             Score: 7.52015004717\n",
      "tomorrow night       Score: 6.59385732977\n",
      "rum runners          Score: 6.33199893985\n",
      "sounds good          Score: 6.30284299504\n",
      "\n",
      "Channel:  recreation\n",
      "\n",
      "miller fields        Score: 8.5083861035\n",
      "sunday miller        Score: 4.78438518916\n",
      "soccer sunday        Score: 4.10924901114\n",
      "fields 5pm           Score: 3.77685479516\n",
      "let know             Score: 3.32321840368\n",
      "armory fields        Score: 3.16872797977\n",
      "play sunday          Score: 2.6602341385\n",
      "fields closed        Score: 2.57576408461\n",
      "looks like           Score: 2.55836785092\n",
      "make tomorrow        Score: 2.49293243197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_channel_top('social')\n",
    "display_channel_top('recreation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Beer Garden' is a favorite of our MSA class. More importantly, it is a place mentioned frequently in the *social* channel, but not so much elsewhere. Hence, it surfaces as a top keyword for that channel. We can also surmise that people ask 'who is going tonight and wants to join?' followed by ..'I'll let you know' ..'sounds good.' You can almost hear a typical conversation taking place in this channel.\n",
    "\n",
    "Comparatively, in the recreation channel, the most popular topic is talking about playing soccer sunday at 5pm at Miller fields. Another location is Armory fields, but with a much lower scores, we see Miller fields is primarily where people play. Armory field is only played in when Miller fields is closed. An interesting question I can tackle is how I can distinguish between usage of Miller for Miller fields and Miller Lite. Now I got ideas poppin'.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel:  linearalgebra\n",
      "\n",
      "factor analysis      Score: 1.91259225706\n",
      "principal components Score: 1.87123950889\n",
      "correlation covariance Score: 1.70710678119\n",
      "dr race              Score: 1.56956155258\n",
      "worksheet quiz       Score: 1.46632668397\n",
      "grades don           Score: 1.39226311039\n",
      "linear algebra       Score: 1.34927756873\n",
      "total variance       Score: 1.31380841384\n",
      "does know            Score: 1.26094372813\n",
      "linearly independent Score: 1.22445651981\n",
      "\n",
      "Channel:  logisticregression\n",
      "\n",
      "logistic regression  Score: 6.1850438952\n",
      "odds ratio           Score: 3.60502587628\n",
      "need know            Score: 3.31621266659\n",
      "ratio test           Score: 3.0764583422\n",
      "odds ratios          Score: 2.97734414536\n",
      "training data        Score: 2.69870342597\n",
      "dr simmons           Score: 2.69276540582\n",
      "likelihood ratio     Score: 2.6202610932\n",
      "common odds          Score: 2.51817439189\n",
      "quasi complete       Score: 2.21223542475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_channel_top('linearalgebra')\n",
    "display_channel_top('logisticregression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we do some academics? 'Factor Analysis' and 'Principal Components' are top keywords in *linearalgebra*, whereas there are many words related to odds and ratio in *logisticregression*. You can even see who teaches these courses - Dr. Race in LinAlg, Dr. Simmons in Logistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel:  foodies\n",
      "\n",
      "foodie outing        Score: 2.64320932962\n",
      "want dinner          Score: 2.25457460504\n",
      "restaurant week      Score: 2.07366157262\n",
      "wants join           Score: 2.06198535522\n",
      "let know             Score: 2.04065156877\n",
      "thinking going       Score: 2.0\n",
      "want try             Score: 1.96772555601\n",
      "really good          Score: 1.78485000765\n",
      "outing tonight       Score: 1.6940231452\n",
      "dinner tonight       Score: 1.68187118087\n",
      "\n",
      "Channel:  _practicum_tech_leads\n",
      "\n",
      "u2r2nanuc thanks     Score: 4.0\n",
      "practicum server     Score: 3.7890372446\n",
      "enterprise miner     Score: 2.51180867934\n",
      "does know            Score: 2.33347405574\n",
      "tech leads           Score: 2.170148842\n",
      "great idea           Score: 2.0\n",
      "think supposed       Score: 2.0\n",
      "using git            Score: 2.0\n",
      "want use             Score: 1.93286781486\n",
      "backup drive         Score: 1.81054101571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_channel_top('foodies')\n",
    "display_channel_top('_practicum_tech_leads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "Foodies is pretty self-explanatory. *u2r2nanuc* is a user-name that is masked for privacy reasons, but is the person who is in charge of the servers, managing transfer portal security. This is a channel where practicum technical leads ask questions and share knowledge. It seems SAS enterprise miner has been talked about a lot, as well as using git, and how to backup drives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel:  python\n",
      "\n",
      "pip install          Score: 5.48826627611\n",
      "https www            Score: 4.11172314471\n",
      "nltk download        Score: 3.98470560605\n",
      "command line         Score: 3.96094126756\n",
      "dr healey            Score: 3.95008169731\n",
      "weather underground  Score: 3.80634314124\n",
      "lunch learn          Score: 3.33454701845\n",
      "http www             Score: 3.26123361693\n",
      "looks like           Score: 3.15393624544\n",
      "let know             Score: 2.99492069591\n",
      "\n",
      "Channel:  datamining\n",
      "\n",
      "data mining          Score: 3.48222499158\n",
      "10 000               Score: 3.34900079537\n",
      "misclassification rate Score: 2.13994929403\n",
      "makes sense          Score: 2.06049634364\n",
      "far apart            Score: 2.0\n",
      "line flattens        Score: 2.0\n",
      "person person        Score: 2.0\n",
      "thanks u1ks1bfed     Score: 2.0\n",
      "thanks u1l6yqusu     Score: 2.0\n",
      "measure impurity     Score: 1.96904987881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_channel_top('python')\n",
    "display_channel_top('datamining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yessssss I love python and data mining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To summarize, we have found phrases that characterize the conversations in each channel. In the [next section](Part 3 - Channel Clustering.ipynb), we will find groupings of these channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out further content ###\n",
    "> Part 1: [Analyzing Trends by Time](Part 1 - Analyzing Trends by Time.html)\n",
    "\n",
    "> **Part 2: [Discovering Channel Keywords]**\n",
    "\n",
    "> Part 3: [Channel Clustering](Part 3 - Channel Clustering.html)\n",
    "\n",
    "> Part 4: [Sentiment Analysis](Part 4 - Sentiment Analysis.html)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
