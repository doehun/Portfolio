{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Great idea <@U1M4QJD4P> \n",
       "1                   Great idea  to have this new channel!\n",
       "2       So we can download the sponsor's files to our ...\n",
       "3       I think we're not supposed to but you could as...\n",
       "4       Did you guys see the size of those servers! Ho...\n",
       "5              I know! They must eat through AA batteries\n",
       "6       If you just put everything on google drive, yo...\n",
       "7        Good point, then I can access it on my phone app\n",
       "8       I'm actually very interested in info sec. Goin...\n",
       "9       I am getting the specs for all of my team's la...\n",
       "10                    to be able to troubleshoot remotely\n",
       "11      please enlighten me on what is going on. have ...\n",
       "12                            <@U1KS4UE2F>: You haven't? \n",
       "13      Ed, yeah have you set up your disaster recover...\n",
       "14                                          :smiling_imp:\n",
       "15      I'm confused. I haven't heard from John at all...\n",
       "16                                               :scream:\n",
       "17                                     lolll u guys :joy:\n",
       "18                               :rage::rage::rage::rage:\n",
       "19      <@U1Q5GHC2J> <@U1KS4UE2F> no lol, don't have a...\n",
       "20      I hacked into the server and took our groups' ...\n",
       "21      I have a teammate using ESET Smart Security 9 ...\n",
       "22      hm. I have no experience with ESET.  maybe try...\n",
       "23       Ok I'll ask her to try that and see what happens\n",
       "24      Anyone have 'disabled' plug ins show up? Are t...\n",
       "25                          I have no idea but I THINK so\n",
       "26                        im gonna send mine in with them\n",
       "27      I had the same thing for windows media player....\n",
       "28      Disabled or up to date are acceptable if I rec...\n",
       "29      Having issues logging into my system, when I t...\n",
       "                              ...                        \n",
       "6376    @shelby  Is it a membership system or you can ...\n",
       "6377    <@U1KQGK2CB>: i do a membership subscription c...\n",
       "6378    Hey everyone!  Not sure if this was mentioned ...\n",
       "6379    Just signed up for 6.30 yoga flow today! Anyon...\n",
       "6380    I have to skip this week, coz I have guests in...\n",
       "6381                                I'm in! <@U1KR9BP0E> \n",
       "6382    I'll be at the yoga class at 5:15 if anyone el...\n",
       "6383             <@U1KQGK2CB>: do they provide yoga mats?\n",
       "6384    <@U1L9DD1NY>: I won't be able to make it today...\n",
       "6385                 <@U1KQGK2CB>: is it at the NCSU gym?\n",
       "6386                                Yea they provide mats\n",
       "6387    Thanks <@U1L7E8GSU> but I'll just get one from...\n",
       "6388    <@U1L9DD1NY>: Yeah mats, blocks, and bands are...\n",
       "6389    Restorative Yoga at 5:15pm - anyone wanting to...\n",
       "6390    If we get to a good stopping point in our proj...\n",
       "6391      I'm going to cycling instead today <@U1L0V1X61>\n",
       "6392       Does anybody know when the next yoga class is?\n",
       "6393                      Right on.. Thanks <@U1KQGK2CB> \n",
       "6394    Thanks <@U1KQGK2CB> . I am going for the Yoga ...\n",
       "6395    Depending on communications training tomorrow ...\n",
       "6396    I am going for Power Yoga today at 5:15pm, in ...\n",
       "6397    Here is Maggie's schedule for August 1-August ...\n",
       "6398                                      Thanks @logan !\n",
       "6399    Hitting up Indigo Hot Yoga tomorrow at 9am (lo...\n",
       "6400    I'm doing yoga flow today at 5:15 if anybody i...\n",
       "6401    I signed up for \"gentle yoga\" for tomorrow at ...\n",
       "6402    I'm going to power yoga at 5:15 today if anyon...\n",
       "6403    heading to wednesday flow at Blue Lotus Yoga (...\n",
       "6404     Signed up for yoga flow tomorrow morning at 6:30\n",
       "6405    has anyone taken yoga flow with Kristen before...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"concatenated.csv\")\n",
    "text = data['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(text[2].split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'computers', u'download', u'files', u'right', u'sponsor']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    "        \n",
    "#this can take some time\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "import os\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "#model = gensim.models.Word2Vec(sentences, min_count = 1)\n",
    "#pass in iterator\n",
    "\n",
    "class MySentences( object ):\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                token = line.split()\n",
    "                yield token\n",
    "\n",
    "\n",
    "\n",
    "sentences = MySentences('txt_data')\n",
    "\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, min_count = 1, workers = 4)\n",
    "model.save('mymodel')\n",
    "\n",
    "\n",
    "#model.save('slackmodel')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"All_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
